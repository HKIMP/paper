用来训练 tranformer 模型中的 encoder网络？


自编码 自回归？

BPE byte pair encoding ?

pooler?
普通的bert后边也有 pooled_output吗

两个任务：
1. 预测被遮挡的单词
2. 预测下一个句子

token_id默认是什么 都是一句话吗

attention_mask作用


