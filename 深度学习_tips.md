1. 为什么需要 激活 层啊？？
2. mini_batch  sgd 是怎么做的 怎么体现sgd的随机 batch和sgd minibatch 怎么回事
3. 各种优化算法：momentum带冲量的优化器。。。每种优化适用场景
4. bert的预训练是啥意思啊
5. 什么是预训练 什么是微调
6. character CNN
7. Glove 里面没有的词怎么办啊
8. pytorch packsequence
9. 交叉熵 分类问题的损失函数 sigmoid softmax
10. sklearn test_split
11. 数据集shuffle 每一次minibatch都不一样?
12. layer norm, batch_normalizaion dropout（李课） batch_norm 一般放到激活函数之后？
13. Pytorch 交叉熵 公式不一样？？？
14. l2 norm





### 优化

bgd sgd mini-batch_gd moonentum adagrad rmsprop adam